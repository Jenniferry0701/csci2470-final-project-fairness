,lambda_reg,model_type,metrics
0,0.01,Adversary,"{'accuracy': 0.8158333333333333, 'precision': 0.6570996978851964, 'recall': 0.3313023610053313, 'f1': 0.44050632911392407, 'statistical_parity_difference': [-0.02, -0.17], 'average_odds_difference': [-0.01, -0.21], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 2.3671236141316165, 'df_bias_amplification': 0.4212134650763031}"
1,0.01,Adversary with Prejudice Remover,"{'accuracy': 0.8165, 'precision': 0.6476323119777159, 'recall': 0.3541507996953541, 'f1': 0.45790251107828656, 'statistical_parity_difference': [-0.03, -0.13], 'average_odds_difference': [-0.02, -0.19], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 2.3353749158170363, 'df_bias_amplification': 0.3894647667617228}"

2,0.1,Adversary,"{'accuracy': 0.8186666666666667, 'precision': 0.6666666666666666, 'recall': 0.3427265803503427, 'f1': 0.45271629778672035, 'statistical_parity_difference': [-0.03, -0.19], 'average_odds_difference': [-0.03, -0.24], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 2.606796467397037, 'df_bias_amplification': 0.6608863183417237}"
3,0.1,Adversary with Prejudice Remover,"{'accuracy': 0.8195, 'precision': 0.6866883116883117, 'recall': 0.32216298552932215, 'f1': 0.4385692068429238, 'statistical_parity_difference': [-0.01, -0.17], 'average_odds_difference': [0.0, -0.21], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 2.772588722239781, 'df_bias_amplification': 0.8266785731844677}"

4,0.5,Adversary,"{'accuracy': 0.81, 'precision': 0.5997693194925029, 'recall': 0.39603960396039606, 'f1': 0.47706422018348627, 'statistical_parity_difference': [-0.03, -0.14], 'average_odds_difference': [-0.02, -0.21], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 4.060443010546419, 'df_bias_amplification': 2.1145328614911056}"
5,0.5,Adversary with Prejudice Remover,"{'accuracy': 0.8168333333333333, 'precision': 0.6671875, 'recall': 0.32520944402132523, 'f1': 0.43727598566308246, 'statistical_parity_difference': [-0.02, -0.24], 'average_odds_difference': [-0.01, -0.3], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 2.4277482359480516, 'df_bias_amplification': 0.4818380868927381}"

6,1.0,Adversary,"{'accuracy': 0.8043333333333333, 'precision': 0.6230088495575221, 'recall': 0.26808834729626807, 'f1': 0.37486687965921195, 'statistical_parity_difference': [0.04, -0.01], 'average_odds_difference': [0.08, -0.01], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 3.5263605246161607, 'df_bias_amplification': 1.5804503755608472}"
7,1.0,Adversary with Prejudice Remover,"{'accuracy': 0.8146666666666667, 'precision': 0.6628849270664505, 'recall': 0.3115003808073115, 'f1': 0.42383419689119173, 'statistical_parity_difference': [-0.03, -0.18], 'average_odds_difference': [-0.02, -0.23], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 2.0301704926730526, 'df_bias_amplification': 0.08426034361773915}"

8,2.0,Adversary,"{'accuracy': 0.8091666666666667, 'precision': 0.6567164179104478, 'recall': 0.26808834729626807, 'f1': 0.3807463493780422, 'statistical_parity_difference': [-0.03, -0.12], 'average_odds_difference': [-0.03, -0.16], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 4.025351690735149, 'df_bias_amplification': 2.0794415416798353}"
9,2.0,Adversary with Prejudice Remover,"{'accuracy': 0.7913333333333333, 'precision': 0.6196078431372549, 'recall': 0.12033511043412033, 'f1': 0.20153061224489796, 'statistical_parity_difference': [0.07, 0.32], 'average_odds_difference': [0.12, 0.29], 'equal_opportunity_difference': [0, 0], 'differential_fairness': 4.647134385087123, 'df_bias_amplification': 2.70122423603181}"
